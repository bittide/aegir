# Copyright 2020 The TensorFlow Datasets Authors, Licensed under the Apache
# License, Version 2.0
#
# Modifications Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# -*- coding: utf-8 -*-
# pylint: disable=pointless-string-statement
"""Copy of tensorflow/datasets.

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10YDCc-6MCkfgRM4oBgb4IiBiWtDt1eSn?resourcekey=0-oKVKrT1ZjGI2ElnBA8iJ9Q

# Training a neural network on MNIST with Keras

This simple example demonstrates how to plug TensorFlow Datasets (TFDS) into a
Keras model.

Copyright 2020 The TensorFlow Datasets Authors, Licensed under the Apache
License, Version 2.0

<table class="tfo-notebook-buttons" align="left"> <td> <a target="_blank"
  href="https://www.tensorflow.org/datasets/keras_example"><img
  src="https://www.tensorflow.org/images/tf_logo_32px.png" />View on
  TensorFlow.org</a> </td> <td> <a target="_blank"
  href="https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/keras_example.ipynb"><img
  src="https://www.tensorflow.org/images/colab_logo_32px.png" />Run in Google
  Colab</a> </td> <td> <a target="_blank"
  href="https://github.com/tensorflow/datasets/blob/master/docs/keras_example.ipynb"><img
  src="https://www.tensorflow.org/images/GitHub-Mark-32px.png" />View source on
  GitHub</a> </td> <td> <a
  href="https://storage.googleapis.com/tensorflow_docs/datasets/docs/keras_example.ipynb"><img
  src="https://www.tensorflow.org/images/download_logo_32px.png" />Download
  notebook</a> </td> </table>
"""
import os
import os.path
import pickle

import tensorflow as tf
import tensorflow_datasets as tfds


def train_mnist():
  """## Step 1: Create your input pipeline

    Start by building an efficient input pipeline using advices from:
    * The [Performance tips](https://www.tensorflow.org/datasets/performances)
    guide

    * The [Better performance with the `tf.data`
    API](https://www.tensorflow.org/guide/data_performance#optimize_performance)
    guide

    ### Load a dataset

    Load the MNIST dataset with the following arguments:

    * `shuffle_files=True`: The MNIST data is only stored in a single file, but
    for
    larger datasets with multiple files on disk, it's good practice to shuffle
    them when training.

    * `as_supervised=True`: Returns a tuple `(img, label)` instead of a
    dictionary
    `{'image': img, 'label': label}`.

    """

  (ds_train, ds_test), ds_info = tfds.load(
      'mnist',
      split=['train', 'test'],
      shuffle_files=True,
      as_supervised=True,
      with_info=True,
  )
  """### Build a training pipeline

    Apply the following transformations:

    * `tf.data.Dataset.map`: TFDS provide images of type `tf.uint8`, while the
    model
    expects `tf.float3

    2`. Therefore, you need to normalize images.

    * `tf.data.Dataset.cache` As you fit the dataset in memory, cache it before
    shuffling for a better performance.<br/>

    __Note:__ Random transformations should be applied after caching.

    * `tf.data.Dataset.shuffle`: For true randomness, set the shuffle buffer to
    the
    full dataset size.<br/>

    __Note:__ For large datasets that can't fit in memory, use
    `buffer_size=1000`
    if your system allows it.

    * `tf.data.Dataset.batch`: Batch elements of the dataset after shuffling to
    get
    unique batches at each epoch.

    * `tf.data.Dataset.prefetch`: It is good practice to end the pipeline by
    prefetching [for
    performance](https://www.tensorflow.org/guide/data_performance#prefetching).
    """

  def normalize_img(image, label):
    """Normalizes images: `uint8` -> `float32`."""
    return tf.cast(image, tf.float32) / 255., label

  ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
  ds_train = ds_train.cache()
  ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)
  ds_train = ds_train.batch(128)
  ds_train = ds_train.prefetch(tf.data.AUTOTUNE)
  """### Build an evaluation pipeline

    Your testing pipeline is similar to the training pipeline with small
    differences:

    * You don't need to call `tf.data.Dataset.shuffle`.

    * Caching is done after batching because batches can be the same between
    epochs.
    """

  ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
  ds_test = ds_test.batch(128)
  ds_test = ds_test.cache()
  ds_test = ds_test.prefetch(tf.data.AUTOTUNE)
  """## Step 2: Create and train the model

    Plug the TFDS input pipeline into a simple Keras model, compile the model,
    and
    train it.
    """

  model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(10)
  ])

  # Add a checkpointing callback.
  model_dir = os.path.join(
      os.environ.get('MODEL_DIR', None) or os.path.dirname(__file__),
      'mnist_model')
  ckpt_file = os.path.join(model_dir, 'mnist_ckpt')
  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
      filepath=ckpt_file,
      save_weights_only=True,
      mode='max',
      save_best_only=True)

  model.compile(
      optimizer=tf.keras.optimizers.Adam(0.001),
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
  )

  model.fit(
      ds_train,
      epochs=6,
      validation_data=ds_test,
      callbacks=[model_checkpoint_callback])
  """## Step3: Save the model"""
  model.save(model_dir)
  model.summary()

  NUM_LAYERS = 3
  pickled_file = os.path.join(model_dir, 'mnist_weights.pickle')
  with open(pickled_file, 'wb') as weights_file:
    weights = []
    biases = []
    for layer_idx in range(0, NUM_LAYERS):
      layer = model.get_layer(index=layer_idx)
      result = layer.get_weights()
      if result:
        w, b = result
        weights.append((list(w.shape), [float(x) for x in w.flatten()]))
        biases.append((list(b.shape), [float(x) for x in b.flatten()]))
    pickle.Pickler(weights_file).dump({
        'weight_shapes': [s for s, _ in weights],
        'bias_shapes': [s for s, _ in biases],
        'weights': [w for _, w in weights],
        'biases': [b for _, b in biases],
    })


if __name__ == '__main__':
  train_mnist()
